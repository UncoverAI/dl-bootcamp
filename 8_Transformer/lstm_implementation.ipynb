{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Sentiment Classification\n",
    "\n",
    "In this notebook, we perform sentiment classification using an LSTM model and the `sentiment_classification` dataset.\n",
    "\n",
    "### Prerequisites:\n",
    "1. Download the `sentiment_classification` dataset from [Link].\n",
    "2. Save the dataset in the following directory structure: 8_Transformer/sentiment_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LSTM_Sentiment_Classification(L.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to convert LSTM output to sentiment scores\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, (hn, cn) = self.lstm(input)\n",
    "\n",
    "        # Use the hidden state from the last time step\n",
    "        final_output = lstm_out[:, -1, :]  # [batch_size, hidden_size]\n",
    "\n",
    "        # Fully connected layer to classify sentiment\n",
    "        prediction = self.fc(final_output)  # [batch_size, output_size]\n",
    "        return prediction\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "\n",
    "        # Forward pass\n",
    "        output_i = self.forward(input_i)\n",
    "\n",
    "        # Cross entropy loss for multi-class classification\n",
    "        loss = F.cross_entropy(\n",
    "            output_i, label_i.argmax(dim=1)\n",
    "        )  # Use argmax to get class indices\n",
    "\n",
    "        # Log the training loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "\n",
    "        # Forward pass\n",
    "        output_i = self.forward(input_i)\n",
    "\n",
    "        # Calculate cross entropy loss for multi-class classification\n",
    "        loss = F.cross_entropy(\n",
    "            output_i, label_i.argmax(dim=1)\n",
    "        )  # Use argmax to get class indices\n",
    "\n",
    "        # Log the loss\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(output_i, dim=1)  # Get predicted classes\n",
    "        accuracy = (\n",
    "            (preds == label_i.argmax(dim=1)).float().mean()\n",
    "        )  # Compare with one-hot encoding\n",
    "        self.log(\"test_accuracy\", accuracy)\n",
    "\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": accuracy}\n",
    "\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data_path, glove_embeddings, max_length=50):\n",
    "        self.data = pd.read_csv(data_path, encoding=\"unicode_escape\")\n",
    "        self.data[\"text\"].fillna(\"\", inplace=True)\n",
    "        self.texts = self.data.get(\"text\")\n",
    "        self.labels = self.data.get(\"sentiment\")\n",
    "        self.glove_embeddings = glove_embeddings\n",
    "        self.max_length = max_length  # Maximum length for padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            print(\n",
    "                f\"Warning: Expected string for text at index {idx}, but got {type(text).__name__}: {text}\"\n",
    "            )\n",
    "            return torch.zeros(self.max_length, 300), torch.zeros(3)\n",
    "\n",
    "        label_encoding = torch.zeros(3)\n",
    "        if label == \"negative\":\n",
    "            label_encoding[0] = 1\n",
    "        elif label == \"neutral\":\n",
    "            label_encoding[1] = 1\n",
    "        elif label == \"positive\":\n",
    "            label_encoding[2] = 1\n",
    "\n",
    "        embeddings = []\n",
    "        for word in text.lower().split():\n",
    "            embedding = self.glove_embeddings.get(word, torch.zeros(300))\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        # Pad the embeddings list to max_length\n",
    "        if len(embeddings) < self.max_length:\n",
    "            padding = [torch.zeros(300)] * (self.max_length - len(embeddings))\n",
    "            embeddings.extend(padding)\n",
    "        elif len(embeddings) > self.max_length:\n",
    "            embeddings = embeddings[: self.max_length]\n",
    "\n",
    "        # Convert list of embeddings to a tensor\n",
    "        embeddings_tensor = torch.stack(embeddings)\n",
    "\n",
    "        return embeddings_tensor, label_encoding\n",
    "\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=\"float32\")\n",
    "            embeddings[word] = torch.tensor(vector)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"sentiment_classification/glove.42B.300d.txt\")\n",
    "\n",
    "training_data = SentimentDataset(\n",
    "    \"sentiment_classification/data/train.csv\", glove_embeddings\n",
    ")\n",
    "test_data = SentimentDataset(\"sentiment_classification/data/test.csv\", glove_embeddings)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "classifier = LSTM_Sentiment_Classification(\n",
    "    embedding_dim=300, hidden_size=128, output_size=3, num_layers=1\n",
    ")\n",
    "classifier.train()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "trainer.fit(classifier, train_dataloader)\n",
    "trainer.test(classifier, test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
