{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA implementation for finetuning a model trained on MNIST (just digits) for EMNIST (letters)\n",
    "inspired by: https://github.com/hkproj/pytorch-lora, https://www.youtube.com/watch?v=PXWYUTMt-AU\n",
    "\n",
    "paper referred to: https://arxiv.org/abs/2106.09685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST and EMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST datasets\n",
    "mnist_full_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(mnist_full_train))\n",
    "val_size = len(mnist_full_train) - train_size\n",
    "mnist_trainset, mnist_valset = torch.utils.data.random_split(mnist_full_train, [train_size, val_size])\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_val_loader = torch.utils.data.DataLoader(mnist_valset, batch_size=64, shuffle=False)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=True)\n",
    "\n",
    "# EMNIST datasets\n",
    "emnist_full_train = datasets.EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(emnist_full_train))\n",
    "val_size = len(emnist_full_train) - train_size\n",
    "emnist_trainset, emnist_valset = torch.utils.data.random_split(emnist_full_train, [train_size, val_size])\n",
    "\n",
    "emnist_train_loader = torch.utils.data.DataLoader(emnist_trainset, batch_size=64, shuffle=True)\n",
    "emnist_val_loader = torch.utils.data.DataLoader(emnist_valset, batch_size=64, shuffle=False)\n",
    "emnist_testset = datasets.EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)\n",
    "emnist_test_loader = torch.utils.data.DataLoader(emnist_testset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some information about and examples from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Dataset:\n",
      "Training set size: 48000\n",
      "Validation set size: 12000\n",
      "Test set size: 10000\n",
      "\n",
      "EMNIST Dataset:\n",
      "Training set size: 99840\n",
      "Validation set size: 24960\n",
      "Test set size: 20800\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6QAAADJCAYAAAA0Pf/EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz30lEQVR4nO3dd3RVdbbA8X0JkERCgIQqHUPUkFCEAYkBIuhQhdgQAQV8KmMZ9Y1gp8oCYWQEBxyxgAqovKFEh/I0FjAgNpAOUoOUACmQBJAAct4fTO6bXyH3EpKclO9nrazl/mWfe8+N53fv+XHu3sfjOI4jAAAAAAAUswpu7wAAAAAAoHxiQQoAAAAAcAULUgAAAACAK1iQAgAAAABcwYIUAAAAAOAKFqQAAAAAAFewIAUAAAAAuIIFKQAAAADAFSxIAQAAAACuYEEKAAC8UlJSxOPxyHvvvef2rgAAygEWpACAS3rvvffE4/Fc8ue7777z5uaNPfjgg9bHevHFF7056enp3vGhQ4eKx+ORli1biuM4xnYej0cef/xxb5y3YHr11VeVvJSUFBk2bJhcc801EhQUJHXr1pXOnTvLmDFj/HoteT9NmjS55N9j5cqV+W778ccf+/V3BQAAF1V0ewcAACXf+PHjpWnTpsZ4RESEEgcFBcmiRYvkjTfekMqVKyu/++ijjyQoKEjOnDljfY7NmzfL4sWL5c4777zs/du9e7f84Q9/kODgYHnggQekSZMmkpqaKuvXr5fJkyfLuHHjpHPnzjJ37lxluwcffFDat28vDz/8sHcsJCTE5/M98cQT8oc//MEY79ix42XvOwAA5RkLUgCATz179pR27dr5zOvRo4d8+umnsmLFCunXr593/Ntvv5V9+/bJnXfeKYsWLTK2Cw4OloYNG8r48ePljjvuEI/Hc1n799prr8nJkydlw4YN0rhxY+V3x44dExGRZs2aSbNmzZTf/elPf5JmzZrJ4MGDL+v5OnXqJHfddddlbQMAAEx8ZRcAUGjq168vnTt3lg8//FAZnz9/vsTExEh0dLR1uwoVKshLL70kmzZtkiVLllz28+7Zs0caNGhgLEZFRGrXrn3Zj3el5syZIx6PR2bPnq2MT5w4UTwejyxfvtw79uqrr0psbKyEh4dLcHCwtG3bVhYuXGg8Zt5Xl//5z39KVFSUBAcHS8eOHWXz5s0iIjJr1iyJiIiQoKAgiY+Pl5SUFGX7+Ph4iY6OlnXr1klsbKwEBwdL06ZN5c033/TrNe3YsUPuuusuCQsLk6CgIGnXrp18+umnSs65c+dk3Lhx0rx5cwkKCpLw8HCJi4uTpKQkv54DAFD+sCAFAPiUlZUl6enpyk9GRoY1d+DAgfKvf/1LTp48KSIi58+fl3/+858ycODAfJ9j4MCB0rx5cxk/fry1ljQ/jRs3lgMHDshXX311WdsVVE5OjvH3SE9P9+73sGHDpE+fPvKXv/xFDhw4ICIXv5I8btw4+a//+i/p1auX97GmT58ubdq0kfHjx8vEiROlYsWKcvfdd8uyZcuM501OTpann35ahgwZImPHjpXt27dLnz59ZObMmfL666/Lo48+KiNHjpS1a9fKAw88YGx//Phx6dWrl7Rt21amTJkiDRo0kEceecRYOOu2bt0qN954o2zfvl2ee+45mTp1qlSpUkUSEhKUf0AYO3asjBs3Tm6++WaZMWOGvPjii9KoUSNZv359gf7OAIBywAEA4BLmzJnjiIj1JzAwUMkVEeexxx5zMjMzncqVKztz5851HMdxli1b5ng8HiclJcUZM2aMIyJOWlqad7shQ4Y4VapUcRzHcd5//31HRJzFixcbj5tn3759jog4f/3rX71jW7ZscYKDgx0RcVq3bu08+eSTTmJionPq1Kl8X1+VKlWcIUOG+P33+Prrry/59xARJzU11ZubmprqhIWFObfeequTm5vrtGnTxmnUqJGTlZWlPObp06eV+OzZs050dLTTtWtXZTzvb75v3z7v2KxZsxwRcerWretkZ2d7x59//nlHRJTcLl26OCLiTJ061TuWm5vrtG7d2qldu7Zz9uxZx3H+/+87Z84cb163bt2cmJgY58yZM96xCxcuOLGxsU7z5s29Y61atXJ69+7tx18SAICLuEIKAPBp5syZkpSUpPysWLHCmlujRg3p0aOHfPTRRyIi8uGHH0psbKz167S6QYMGFegqaYsWLWTDhg0yePBgSUlJkenTp0tCQoLUqVNH3n77bb8fx1+jR482/h5JSUkSFhbmzalbt67379apUyfZsGGDzJ49W0JDQ5XHCg4O9v738ePHJSsrSzp16mS9qtitWzelC3CHDh1EROTOO++UqlWrGuN79+5Vtq9YsaIMHz7cG1euXFmGDx8ux44dk3Xr1llfa2Zmpnz11VfSv39/5cpwRkaGdO/eXXbt2iWHDh0SEZHq1avL1q1bZdeuXfn+/QAAyENTIwCAT+3bt/erqVGegQMHyn333Se//vqrJCYmypQpU/zaLiAgQF566SUZMmSIJCYmyu233+73c0ZGRsrcuXPl999/l23btsnSpUtlypQp8vDDD0vTpk3llltu8fuxfImJifHr8QYMGCDz5s2TZcuWycMPPyzdunUzcpYuXSoTJkyQDRs2SG5urnfc1tipUaNGSlytWjUREWnYsKF1/Pjx48r41VdfLVWqVFHGIiMjReTibXNuvPFG4zl3794tjuPIqFGjZNSoUdbXeezYMalfv76MHz9e+vXrJ5GRkRIdHS09evSQ++67T1q2bGndDgAArpACAApd3759JTAwUIYMGSK5ubnSv39/v7cdNGiQREREFKiWVOTiojYmJkaef/55b33j/PnzL/txCkNGRob89NNPIiKybds2uXDhgvL75ORk6du3rwQFBckbb7why5cvl6SkJBk4cKD1tQcEBFif51LjBfn76fL2ecSIEdarwklJSd7b/3Tu3Fn27Nkjs2fPlujoaHnnnXfkhhtukHfeeeeK9wMAUDZxhRQAUOiCg4MlISFB5s2bJz179pSaNWv6vW3eVdKhQ4fKJ598ckX7kXdVNzU19Yoep6Aee+wxycnJkUmTJsnzzz8v06ZNk7/85S/e3y9atEiCgoLks88+k8DAQO/4nDlzimR/Dh8+LKdOnVKuku7cuVNERPkq8H/Ku1VOpUqV/LoqHBYWJsOGDZNhw4bJyZMnpXPnzjJ27Fh58MEHr/wFAADKHK6QAgCKxIgRI2TMmDGX/JpnfgYPHiwREREybtw4v/KTk5Pl3Llzxnje7VWuvfbay96HK7Vw4UJZsGCBvPLKK/Lcc8/JgAED5KWXXvIuAEUuLr49Ho/8/vvv3rGUlBRJTEwskn06f/68zJo1yxufPXtWZs2aJbVq1ZK2bdtat6ldu7bEx8fLrFmzrAv7tLQ073/rnZdDQkIkIiJC+SoyAAD/iSukAACfVqxYITt27DDGY2NjvVfQdK1atZJWrVoV6PkCAgLkxRdflGHDhvmVP3nyZFm3bp3ccccd3nrF9evXywcffCBhYWHy1FNPFWg/LiU5OVnOnDljjLds2VJatmwpx44dk0ceeURuvvlmefzxx0VEZMaMGfL111/L0KFDZfXq1VKhQgXp3bu3/O1vf5MePXrIwIED5dixYzJz5kyJiIiQTZs2Feo+i1ysIZ08ebKkpKRIZGSkLFiwQDZs2CBvvfWWVKpU6ZLbzZw5U+Li4iQmJkYeeughadasmRw9elTWrl0rBw8elI0bN4qISFRUlMTHx0vbtm0lLCxMfvrpJ1m4cKH3bwAAgI4FKQDAp9GjR1vH58yZc8kF6ZUaPHiwTJgwQfbs2eMz94UXXpAPP/xQVq1aJfPnz5fTp09LvXr1ZMCAATJq1Chp2rRpoe7b66+/bh0fM2aMtGzZUh555BHJzc2VOXPmeJsThYeHy1tvvSX9+vWTV199VZ555hnp2rWrvPvuu/LKK6/IU089JU2bNvUuGItiQVqjRg15//335c9//rO8/fbbUqdOHZkxY4Y89NBD+W4XFRUlP/30k4wbN07ee+89ycjIkNq1a0ubNm2UY+OJJ56QTz/9VD7//HPJzc2Vxo0by4QJE2TkyJGF/loAAGWDxymMjgcAAKBEi4+Pl/T0dNmyZYvbuwIAgBc1pAAAAAAAV7AgBQAAAAC4ggUpAAAAAMAV1JACAAAAAFzBFVIAAAAAgCtYkAIAAAAAXMGCFAAAAADgChakAAAAAABXsCAFAAAAALiCBSkAAAAAwBUsSAEAAAAArmBBCgAAAABwBQtSAAAAAIArWJACAAAAAFzBghQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4ggUpAAAAAMAVLEgBAAAAAK4o8wvSlJQU8Xg88uqrrxbaY65cuVI8Ho+sXLmy0B4TKG7MDcDEvAD+H/MBuIi5ULRK5IL0vffeE4/HIz/99JPbu1LoHn30UalQoYJkZmYq45mZmVKhQgUJDAyUM2fOKL/bu3eveDweeeGFF4pzV1ECleW58Z+Sk5Olf//+Ur9+falcubJUq1ZNOnToIOPHj5ejR4+6vXsoYcr6vBg6dKh4PB7vT2hoqLRq1UqmTp0qubm5bu8eSpiyPh/yLF26VHr06CHh4eESFBQkkZGRMmLECMnIyHB711BClOW5UNbWEyVyQVqWxcXFieM4smbNGmX822+/lQoVKsi5c+eMiZOXGxcXV2z7Cbhl9OjR0rlzZ1m3bp0MHTpU/vGPf8jEiROlRYsWMnXqVImNjXV7F4FiFxgYKHPnzpW5c+fKxIkTJSwsTEaMGCFDhgxxe9eAYjdixAi57bbb5MiRI/Lss8/KjBkz5JZbbpEZM2ZIq1at5JdffnF7F4EiVdbWExXd3oHyJu8gWL16tdx2223e8TVr1kjLli3lt99+k9WrVysHy+rVq6VChQqciKPMW7Bggbz88svSv39/mTt3rlSuXFn5/WuvvSavvfaaS3sHuKdixYoyePBgb/zoo49Khw4dZMGCBfK3v/1Nrr76ahf3Dig+H330kUydOlXuuecemT9/vgQEBHh/N3ToULn55pvl7rvvlvXr10vFipzmomwqa+uJUnuF9OzZszJ69Ghp27atVKtWTapUqSKdOnWSr7/++pLbvPbaa9K4cWMJDg6WLl26yJYtW4ycHTt2yF133SVhYWESFBQk7dq1k08//dTn/pw+fVp27Ngh6enp+eY1atRIGjZsaPyLxpo1a+Smm26S2NhY6+9atGgh1atX97kfQGmdGyIXr47WrFlT3n33XWMxKiJSrVo1GTt2rM/HAXSleV7YVKhQQeLj40XkYm0TcDlK83wYN26c1KhRQ9566y1lMSoi0r59e3n22Wdl8+bNsnDhQp+PBZTWuVDW1hOldkGanZ0t77zzjsTHx8vkyZNl7NixkpaWJt27d5cNGzYY+R988IG8/vrr8thjj8nzzz8vW7Zska5duyr1aFu3bpUbb7xRtm/fLs8995xMnTpVqlSpIgkJCbJkyZJ89+eHH36Q66+/XmbMmOFz3+Pi4uSnn37y1v6cPXtWfvzxR4mNjZXY2Fj59ttvxXEcERE5fvy4bNu2rUReXkfJVFrnxs6dO2Xnzp2SkJAgISEhBXrtwKWU1nmRnz179oiISHh4eIEfA+VTaZ0Pu3btkl9++UX69esnoaGh1pz7779fRC7WmAK+lNa5IFLG1hNOCTRnzhxHRJwff/zxkjnnz593cnNzlbHjx487derUcR544AHv2L59+xwRcYKDg52DBw96x7///ntHRJz//u//9o5169bNiYmJcc6cOeMdu3DhghMbG+s0b97cO/b11187IuJ8/fXXxtiYMWN8vr6ZM2c6IuIkJyc7juM4a9eudUTE2b9/v7Nt2zZHRJytW7c6juM4S5cudUTEmT9/vs/HRdlXlufGJ5984oiIM23aNGX8woULTlpamvJz7ty5fB8L5UtZnheO4zhDhgxxqlSp4j3+d+/e7UycONHxeDxOy5YtfW6P8qUsz4fExERHRJzXXnst37zQ0FDnhhtuyDcHZV9ZnguOU7bWE6X2CmlAQID3K30XLlyQzMxMOX/+vLRr107Wr19v5CckJEj9+vW9cfv27aVDhw6yfPlyEbnYleqrr76S/v37S05OjqSnp0t6erpkZGRI9+7dZdeuXXLo0KFL7k98fLw4juPX1wn/83vfIhcvodevX18aNWok1113nYSFhXkvs5fkAmSUTKV1bmRnZ4uIGFdHs7KypFatWsqP7V8tgfyU1nmR59SpU97jPyIiQl544QXp2LGjz39tB2xK63zIyckREZGqVavmm1e1alXvZwqQn9I6F0TK1nqi1C5IRUTef/99admypQQFBUl4eLjUqlVLli1bJllZWUZu8+bNjbHIyEhv7c3u3bvFcRwZNWqUcfI7ZswYERE5duxYoex3dHS0VK9eXTlIbrrpJhER8Xg80rFjR+V3DRs2lEaNGhXKc6N8KI1zI+8E4+TJk8p4SEiIJCUlSVJSkowcOfKKnwflV2mcF3mCgoK88+Cbb76RAwcOyJo1a6RZs2aF9hwoX0rjfMj7nMhbmF5KTk6Oz0UrkKc0zgWRsrWeKLXtx+bNmydDhw6VhIQEGTlypNSuXVsCAgJk0qRJ3rqay3HhwgURudhKvHv37taciIiIK9rnPBUqVJCOHTt6v9u9Zs0a5Z5AsbGxMnv2bO93wRMSEgrleVE+lNa5cd1114mIGM0BKlasKLfccouIiBw8ePCKnwflU2mdF3kCAgK88wC4UqV1Plx//fUiIrJp06ZL5uzfv1+ys7MlKirqip8PZV9pnQsiZWs9UWoXpAsXLpRmzZrJ4sWLxePxeMfz/vVBt2vXLmNs586d0qRJExER778yV6pUqVg+9OPi4mTFihXy6aefyrFjx7z/oiFy8QB68cUXZfny5fLbb7+V2MvrKJlK69y49tprpXnz5pKYmCjTpk2TKlWqFNlzofwprfMCKAqldT5ERkZKZGSkJCYmyvTp061XQT/44AMREenTp0+R7QfKjtI6F/KUlfVEqf3Kbl6rb+ff3aNERL7//ntZu3atNT8xMVH5zvYPP/wg33//vfTs2VNERGrXri3x8fEya9YsSU1NNbZPS0vLd38ut4V/3kExefJkueqqq6R169be37Vv314qVqwoU6ZMUXIBf5TmuTF27FhJT0+Xhx56SM6dO2f8/j9fE3A5SvO8AApbaZ4Po0ePluPHj8uf/vQn+f3335XfrVu3TiZPnizR0dFy5513+nwsoDTPBZGys54o0VdIZ8+eLf/7v/9rjD/55JPSp08fWbx4sdx+++3Su3dv2bdvn7z55psSFRVl1KCJXLw8HhcXJ4888ojk5ubKtGnTJDw8XJ555hlvzsyZMyUuLk5iYmLkoYcekmbNmsnRo0dl7dq1cvDgQdm4ceMl9/WHH36Qm2++WcaMGeNXIXL79u2lcuXKsnbtWomPj1du3nzVVVdJq1atZO3atVK9enWJjo72+XgoX8rq3Bg4cKBs2bJFJk2aJD/88IMMGDBAmjZtKqdOnZItW7bIRx99JFWrVpUaNWr4/8dCuVFW5wVQEGV1PgwaNEh+/PFHmT59umzbtk0GDRokNWrUkPXr18vs2bMlPDxcFi5cKJUqVfL/j4UyrazOBZEytJ4o7ra+/shr03ypnwMHDjgXLlxwJk6c6DRu3NgJDAx02rRp4yxdutQZMmSI07hxY+9j5bVp/utf/+pMnTrVadiwoRMYGOh06tTJ2bhxo/Hce/bsce6//36nbt26TqVKlZz69es7ffr0cRYuXOjNudI2zXk6duzoiIjzwgsvGL974oknHBFxevbs6ffjoewrL3Nj5cqVzl133eXUq1fPqVSpkhMaGuq0a9fOGTNmjJOamlqQPx3KsLI+L/Ju+wL4o6zPhzyJiYnOrbfe6tSoUcMJDAx0IiIinKefftpJS0sryJ8NZVB5mQtlYT3hcRy+AwcAAAAAKH6ltoYUAAAAAFC6sSAFAAAAALiCBSkAAAAAwBUsSAEAAAAArmBBCgAAAABwBQtSAAAAAIArWJACAAAAAFxR0d9Ej8dTlPuBEohb1OaPOVH+MCfyx5wof5gT+WNOlD/MifwxJ8off+YEV0gBAAAAAK5gQQoAAAAAcAULUgAAAACAK/yuIQUAAAAAlBwVK17+cu7ChQt+jRUXrpACAAAAAFzBghQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4okw3NdJvvsvNioHLFxAQoMSVKlUycurWravEBSmw99eJEyeUOCcnx8jJzc0tsucHgPKkQgXz2oX+uaC/59epU8fYxp/PhfPnzyvxqVOnjJzMzEwl/v33330+LlCYbHNCH9PXINWrVze2qVat2mU/t22bXr16+dw/vWHRli1bjJxt27Ypse1cKjU11WdOQXCFFAAAAADgChakAAAAAABXsCAFAAAAALiiUAu9atasqcT33XefkRMSEqLEtu8wb9682edz6fULffr0MXKioqKUeNWqVUbOihUrlFivV6hdu7axTVHWx/kSExNjjEVHRyvxyZMnjZy5c+cqcXp6euHuGMoM/fgOCwtTYr1eVESkW7duShwaGloo+2K7SfOGDRuU2PZ+kZKSUijPDwBlme18xp/atxo1aiixfm7XpUsXYxs9x1bnlp2drcS29/I1a9YocVZWlpFDHwGIFOx83dYno169ekrcqlUrI0c/F9efu0WLFsY2+phtTvizf/Xr11difQ6LmH109H4ctjFbzoQJE5R42bJlRo7t3M0XrpACAAAAAFzBghQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4osDdefSmQiIi9957rxLrha8iZpGvP0W1/mjQoIExVrlyZSW+7bbbjJy+ffsq8a+//qrEnTp1MrYprIYtBWFrLqCP6TeWto298cYbRg43ly5bbEXttWrVUmJbg6Jbb71ViTt27KjEERERxjbNmzdXYlvRfUHoRfgiIlu3blXixMREI2fKlClKfObMmULZHwAoqfRzHn/OF3r37m3k6Oc4/jRT1J+7Tp06xjb6+Z/tM+rcuXNKnJOTY+TozSg///xzI0f/XPjtt9+MHJRuQUFBShwZGWnkFKThYtWqVY2x+Ph4JW7YsKGRo88t/fi2NSzSx2znPPq5uW3e6Gsyf5oa6eeDtjF9PoqItG7dWon1+ShCUyMAAAAAQCnCghQAAAAA4AoWpAAAAAAAVxS4htT2/WS9PkD/frdtO3++w1xYatasaYwlJCQosf4da1utbElnq98LCwtTYtv/P5Ru+rHatGlTI+fFF19UYr0WQMSswwgMDFRif27cbOPPMWern9Bde+21SqzXgYuIzJs3T4n37t3r83EBoKTSP8NttXCdO3dWYlvfjBo1aihxhw4djBy9HtSf8yC9ZsxWe6b3B8nKyjJy9Nep76+ISP/+/ZU4Li7O5/4tXrzYGMvNzfW5HdyhHwdNmjQxcvTj+/bbbzdyCqu/hX7+cvbsWSPn0KFDSqzPAX9qP239Lnbs2KHEttdgqwX3xTZH9ddgq70+fvy4Evtz3uYPrpACAAAAAFzBghQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4osBNjc6fP2+MJSUlKfEvv/xi5ISHhyuxXrgsUniNhPxpolLamhb5Uzys30RXRCQ7O7sodgcu0RsNiYhUq1ZNiW+66SYjRx+rV6+ekRMcHHyFe1fwpln6drbjXS/o9+fm7wBQWtgax8XExChxq1atjJyuXbsq8Y033mjk6A2LrrrqKiNHfx+2nVNkZmYq8alTp/L9vYhISkqKEm/fvt3IiY2NVWL9dYuY55FXX321kfPHP/5Rib/88ksjJyMjQ4ltrxNFz3Yefs899yjx448/buQ0aNBAif1ppGo7p9CbbdmO3W3btinxDz/8YOSsXLlSifUGRREREcY2t9xyixLXrl3byLnuuuuUuGrVqkaO3lTM9jr1Jkbr1683ckaNGqXEBw8eNHL0v09hzRuukAIAAAAAXMGCFAAAAADgChakAAAAAABXeBw/72hakJow281b9Zsc2+rc9NoI2w1x69atq8RNmzb1uT+219C4cWMl1ve5oLVw/vDnT6/fuHn//v1GzqZNm5R4wYIFRs7SpUvzfdyC7l95VlTHhu1x9ZpR2w2g9ZqZnj17Gjm1atXy+Vw6vT6gKI+LihUvv6zdduPml156SYn//ve/Gzm2OnhfmBP5K8r3y5LMVvOn1/SUVcyJ/BVkTtjq+MeNG6fECQkJRk6zZs2U2HZc6mznAllZWUqcnJxs5OjnGfv27VNivT5TxKwztfW20D+3+vXrZ+QMGDBAiW09FY4dO6bEjzzyiJGzevVqJU5LSzNyCoI5kT9/5oTeX8bWb6awnD17VoltvTV69eqlxL179zZy9NpOfT1Rp04dYxs9R6/zFBHZtWuXEn/xxRdGzpo1a5RYr3kVMef6kSNHjBy97rWw+DMnuEIKAAAAAHAFC1IAAAAAgCtYkAIAAAAAXMGCFAAAAADgisvvIHIZbMW5eqH5J598YuToDXiqVKli5OhF/7YcvUGK7QbQ+vPXr1/fyPHFn2JdW4MLvcA4NTXVyNGbCUyaNMnIOX78uBLbburLDZ9LD/2m3yIiHTt2VGK9wYWIeezaGmP4Iz09XYk/+ugjJdZvIi1SsAYuoaGhxpjerMnWXEBvYGFrnla9enUlLq/NdlA09EZ7r7zyipGjf7bYPutOnz7t87n0xjT+zDXb8U6jlZJL/38cEhJi5OgNi6pVq+bzcWz04+Dw4cNGjt4o8eOPPzZy9HOTnJwcJbad/+nHru2Y1J/b9jr1hjK2zwC9wYytOebPP/+sxIXV1AhXTj+PtZ3XFoTtOBg6dKgS33///UbO1VdfrcRBQUFGjn586+fdBw4cMLbRPxcOHjxo5Hz55ZdKrDc5EjHXE6WxqR5XSAEAAAAArmBBCgAAAABwBQtSAAAAAIArirSG1B+27znrY7aaNX3MVi93yy23KHGXLl2MnJo1a+a7fwWtu9FvAP3VV18ZOStXrlTiVatWGTm//vqrEuv1fSh7bLWVkZGRSmyrrbTVNOj0uWWbf4cOHVLiJUuWKLHtZsr64/hTy2Srk7rmmmuU2Fb3Xbt27ct+7MqVKxs558+fV2Jq7MonvdeAjV6T2adPHyW+++67jW369eunxL/88ouRo9cP2XohREVFKfHWrVuNHL0WtVGjRkZOSkqKEmdnZxs5+lzX3wtE7LWBuDL6+6WtbrJFixZKrNfJi5jHqe0m93rN6OjRo42c77//Xon37dtn5BRVXwr9nEevVbXl2OaNXldq6w8SFhamxPocQemnv7/rfSpEREaMGKHEtnMTne18YePGjUqclJSkxN99952xjZ6j14KKmOcqZRVXSAEAAAAArmBBCgAAAABwBQtSAAAAAIArWJACAAAAAFzhelMjfwQEBBhjjRs3VuKpU6caOd27d1dif5q+6GyFy3rh+xtvvGHk6IXKtoYWtuJllD/68X3DDTcYOR07dlRiWxMvvaGFrenEN998o8SbN282cvSbMP/444/57q+ISN26dZX4pptuMnKOHj2qxHpTLxGRefPmKbGtmD8hIUGJbU1p9IYytudau3atEtMwrOypVauWEjds2NDI6d27txLbGnLpY3fccYcS63NPRCQwMFCJW7ZsaeToDYts9PmmN+uzse2P/llma07Uo0cPJR4+fLiRQ+MXd+jHoD//j/X3XBGRzz//XImXLl1q5OTk5CixrfldUdHf81NTU40c/f1cb04kYp4jxsfHGzl6065Nmzb53B+UPfq8sR3v/sw/vUmqvuawnavoDbnOnj3r87mLcz4WJ66QAgAAAABcwYIUAAAAAOAKFqQAAAAAAFe4XkPqT32oraZu0KBBSqzfqPxSj63TvzuekZGhxLZ6menTpyvxggULjBzqDuAvvf6lf//+Rk5cXJwS2+rcdPv37zfG/vGPfyjxli1bjJzTp08rsV4H0bRpU2ObZ555Rom7du1q5Oj1TI8//riRs2HDBiXW3wtERHr27KnEtnpavVawdevWRs727duVmBrSkkOvtbHVWup1wtWqVTNyBg8erMS2WjNbXU9xKchz+7ONP/WFtsdp0qTJZT8XSq4zZ84YY/q5iT/Hipts51KnTp1SYls9tP669Fo9EZGQkJAr3DuUdPrxs2TJEp/b2Hpg6OcQ9erVM3KuvvpqJX700UeVeMCAAcY2q1evVmJbXw+9tnnjxo1Gjn6+Z+shUtJxhRQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4ggUpAAAAAMAVxd6x4KqrrlLiW2+91cgZNWqUEttuZq43p7A1MLIV6+uysrKUeMqUKUpsK4A+ePCgEtPACFeiatWqStysWTOfOTb6zZK3bt1q5OzatUuJbU0drrnmGiX+8ccfldjWQKJ58+ZKHBoaauTo21WvXt3I2bZtmxLv3bvXyDl58qQSBwYGGjn63PenCRSKh94k67HHHjNy7r33XiW+/vrrfT6OrYHLihUrlNjWMKIg9OPJ9hr0z6jMzEwjZ+bMmUrszw3Ze/XqZeTY5psv+/btM8ZWrlypxLZ9RsmlNzL57LPPjJxFixYpsd4gSKRkNTWy7Yttnvhi+wzgc6H8sTUq/fvf/67E8+bNM3L0dcjNN99s5HTs2FGJo6OjldjWVE9vznfbbbcZOSdOnFDiAwcOGDmTJk1S4lWrVhk5Jb1xI7MRAAAAAOAKFqQAAAAAAFewIAUAAAAAuKJIa0hr1qxpjA0fPlyJhw0bZuTYauh88ade1CY4OFiJ27Rpo8SJiYnGNnq9ju2Gy0ePHlViW91dQVCvWrrZalZatWqlxLaaaVudpE6vtbHVX+q1B7Z5k5qaqsQVK6pvExEREcY2+g2h9W1ERKpVq6bEtnmu1/zZbgCtvwZbHWylSpWMMRQ/22eAXqc/ZMgQn4+zdOlSY0yvd/zyyy+NHL2OurBuFq4f37Ybnuv1QhkZGUbOyy+/rMT+vL/r2xSUrTavNN5MHf9P/39qqwE+fPiwEhfWuUlppH8eF/Q8EqWb/r6blpZm5OhjmzZtMnI++OADJa5Ro4YS6zWmImb/gcaNGxs5+uPYPlf1Oli9P4GIyPTp05VY78fhNq6QAgAAAABcwYIUAAAAAOAKFqQAAAAAAFcUaQ1pQkKCMfb0008rse1ehMX5Pf7KlSsrsV4L9Mc//tHYRq/TKMoa0uzsbCX++OOPjRz9vmK2+yyhZLDVkOr3qrLNCX/ul6bfi7dv375Gzu7du5X4u+++M3J+/vlnJdZrPaOiooxtbPus0+vubPdO1F+n7b5ZycnJSmy7t1f9+vWVuCD3rcOVsx0Xeh2N7f1ev0em/rkhYr7PFWd9vV6j7E/NcmEdg/QRAC6fbY7q52623gflucYWl2Z7H9brTPV4586dxjbz589XYr1eVMTsK2I7txs5cqQSv/TSS0ZOjx49lPjxxx83cvTa2OK8JzFXSAEAAAAArmBBCgAAAABwBQtSAAAAAIArWJACAAAAAFxRpE2NvvjiC2Ns2bJlSmxrfGRrEuRLQRsh+douPDy8QI/bpEmTAm3nS/PmzY0xvbh6xowZRg43PC8ZbMeb3kihsJp6hYSEGGNZWVlKnJGRYeT4ar5ia7Dkzz7r23Xu3NnnNjatW7dWYltzpBMnTijxli1bjBz9b4HCZ2v84M/NuJcvX67E+/fv9+uxi8v111+vxA0aNDByzpw5o8S2hnQ0KEJRsr1X683vSjrbZ4s/Tf50tuZ3N9xwg8+cI0eOKDHnUihM+meA3gjJNrZjxw4j55prrlHiO+64w8hp166dEj/11FNGjt4cydZYsqhwhRQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4okhrSG11P6NGjVLiXbt2GTkDBw5UYtuNYvUx2w2NC1KLV1j1ewXhzw1oc3NzjbFff/31sh8H7rAdp4VVM63/fz916pSRo9ce2OoVdHrNUUHqd2zbxcTEGDn169f3+Th6nU/VqlWNnAMHDiix7b3In1pGXJmDBw8aY3otZdu2bY2c4cOHK/HGjRuNnE8++USJMzMzjZzCqvcKCgpSYv1zzDav9RuMz507t1D2BWWfrbY4JydHic+dO2fkBAYGKnGLFi2MnKioKCW2nYP56iNQlPTPierVqxs5+uuy5ehSUlKMsVWrVimxracCNaMoaU6fPm2MjRs3ToltvTV69+6txJ06dTJyGjVqpMTUkAIAAAAAyjwWpAAAAAAAV7AgBQAAAAC4ggUpAAAAAMAVRdrUyNZcRy8sHz9+vJEzadIkJbY1NbrpppuU+H/+53+MHFujiaLgTxMhW5OC48ePK/GJEyeMnOzsbCW2NcbQC/PdbEgAlX4M1qlTx8jp0qVLvtsUlK3phV4Mb2tQ1KRJEyXu3LmzErdu3drYprBuVG4b88XWdGL9+vVKrDc5ErE3CEPhsr3v6e9h99xzj5HTqlUrJX7zzTeNnAkTJihxcnKykfPKK68ocVZW1qV39t9s82/QoEFK3LdvXyU+c+aMsc3LL7+sxLbGWoCI+Zlta0innzs1bNjQyAkODlZi/b1cRKRZs2ZKbHvvLklNjWxN//TXFRISYuTor2HPnj1GztatW5XY9pkJlAaHDh1SYr2pnohIz549lbhevXpGjn4+ansc2+d6YeAKKQAAAADAFSxIAQAAAACuYEEKAAAAAHBF8RRZXib9e/y2G9jrNUYBAQEFei5/6j99sdUP7dy5U4m/+OILI2fNmjVKvG3bNiNHr3Oz3Wi+qL7PjZLLn+PWVlej1+vZ5pZe59q9e3clttUu6XU/Ho/HyCmMuSYicvbsWSU+fPiwkTN//nwlzszMNHKotXZHWlqaEg8cONDImTNnjhJ36NDByKldu7YSJyQkGDlt27ZV4oLWiIWHhyuxfnx/9tlnxjb6mK3WGRAx34v0/hIiIt98840S68ekiEhsbKwS6ze5FzF7AixZssTI0evR3Ky3t9V0V61aVYkrVapk5Oh/0y1bthg5mzdvVmLmKEqioKAgJY6MjDRy7rjjDiWOi4szcgrS66M4ley9AwAAAACUWSxIAQAAAACuYEEKAAAAAHAFC1IAAAAAgCtKZFMjvVmFfjNXEZH77rvvsh/X1lRFH8vIyFBiW3MBvVj+448/NnL0RgG7du0ycvRGATRZKXv8ueH5gQMHlLhNmzZGjj/F6PqxbGtYpM+tsLAwI0e/EbJ+I3XbjcpDQ0Pz3Rd/+dOgS39de/fuNXK2b9+uxDSrKDn0Y2P37t1Gzq233qrEUVFRRs7zzz+vxJ06dTJyGjRoUJBdNOjNkN5//30lHjlypLGNrdkd4A/b+6B+nrFu3TojZ8aMGUocExNj5HTt2lWJbedSn3zyiRLrzX9Eiu49NTAwUImrV69u5NiaGOn0OXvixAkjx81mTSh/9GNbRKRevXpKbGtW1qdPHyW+/fbbjZzrr79eiW2NXvU5YXsPSUpKUuLibJrKFVIAAAAAgCtYkAIAAAAAXMGCFAAAAADgCtdrSG3fc54wYYIS9+3b18ipVauWz8fWa5VsNZp6bcS0adOU+LvvvjO20WsnDh48aORQPwR/FaR22Fb7cvjwYSV++eWXjZyvvvpKiT0ej5Gj11H//PPPSnzvvfca2/z5z39WYtvNzHUpKSnGmF57bau51etgjxw5YuQcO3bM5/Oj5NLfP9evX2/kDBgwQIlt9dDVqlUrlP3R62j0um9qlFHUMjMzldhWe71hwwYljoyMNHL0OjZbz4JDhw4psf7ZYtsf/XPMn8812+ePXlMXHR1t5ISEhCixrcdCTk6OEts+b2yfL4A/bMecvp6pUaOGEtv64Tz22GNKXLNmTSOnTp06Sly5cmUj5+zZs0qsf0aJiCQnJyuxvt4REfnll1+MseLCFVIAAAAAgCtYkAIAAAAAXMGCFAAAAADgChakAAAAAABXFHtTI70QuFevXkaOfqNm281k9WJ4vYGRiMjJkyeVePr06UbOxIkTlfi3334zcoDCYjtO/Wn+oG+3Y8cOIycxMVGJFy9ebOScPn3a53Pp9BuK224wbntdOr0xzKJFi4yc8ePHK7GtedNVV13l87mzs7N97g9KN72RUFpampFjGwNKI/3909a4TW8IaWsu16FDByXu1q2bz5wePXoYOWvXrlXivXv3KvG2bduMbfQ5W716dSPnySefVOIbb7zRyAkPD1di22eo3sBlzZo1Ro7emAkll60Bqr6esB1PBWlsZ3uumJgYJe7fv7+R07JlSyXWmxrZ9k9nO7/aunWrEs+cOdPIWbVqlRIfPXrUyNHPpwrSULMocYUUAAAAAOAKFqQAAAAAAFewIAUAAAAAuML1GtLWrVsbObaaUZ1ei5Cenm7kvP3220r87rvvGjnUjKIo6d/Rt92I+9dff813GxHzeNdvgG4bs9VfFoReu6TXZouYN2W21S7p+3PkyBEjR5+P+nOLiJw7d+7SO/tv/tS0AkBpZXuPO3z4sBIvX77cyNHfU231oSEhIUocFxdn5DRp0kSJU1JSlFivexMxP9tCQ0ONnNjYWCWuV6+ekaM7c+aMMbZ582YltvUV0D9XUXLUrFlTie+9916fOS1atDBy9DF9DeIvvf4zLCzMyNHnpF6j/K9//cvYRj9ON23aZORs3LhRiQ8dOmTkFNb5npu4QgoAAAAAcAULUgAAAACAK1iQAgAAAABcwYIUAAAAAOCKYm9qpLM1SNELg23FuklJSUr8xRdfGDkLFixQYtuNpIHiZGvIk5WVpcQ5OTlGzv79+5V42rRpRs6uXbuUuLAaNqSmpirxkiVLjBz9ptFRUVFGjn6j9EWLFhk5tiZGOhoWAYBJbwq3ePFiI+fLL79U4qVLlxo50dHRSnz77bcbOddee60S681jevbsmf/OiojH4zHG9KYzts8xvfGLrcnfBx98oMS2xkcoGWyNhjp16qTEzz33nJFTq1YtJbYdT/6cL+jnZQcPHjRyTpw4ocR6M0oRkZUrVyrxt99+q8Rr1qwxtjl+/LgS2453W6PLsogrpAAAAAAAV7AgBQAAAAC4ggUpAAAAAMAVHsfPgizbd7MLQ1BQkDE2fPhwJd67d6+Ro9eQUh9Q+KjVy19B5oQ/tRKtW7c2cvbs2aPE+vEvUnw3Rra97vDwcCXWb6wuYtaLZ2RkGDkl/Zgr6fvntqL6nEDJxZzIX0mfEwEBAcZYpUqVlDgyMtLI6datmxLr7/m2zzp/6PVytj4jeh3skSNHjJy0tDQlLs7jlDmRP31OVKxotrN59tlnlXjkyJFGTmZmZr6xiFnXaevRkZ2drcTLly83cvTaTlu/i6NHjyqxfk5WXmpBbfyZE1whBQAAAAC4ggUpAAAAAMAVLEgBAAAAAK5gQQoAAAAAcIXrTY1s9CJ72y6W5+Lg4kJhfv4Ka06EhYUpcWhoqJFT0hsC6Q0s/LlBdWmcw8yJ/JX0Bi4ofMyJ/JWFOWFrPlm3bl0ltjWmKQy25jF6E6Nz584ZOXoTmuLEnMifPicK2uwxOTlZiW1NjfRGQ7ZjRWc75nBlaGoEAAAAACixWJACAAAAAFzBghQAAAAA4IoSWUOKkoE6iPwxJ8of5kT+mBPlD3Mif8yJ8oc5kT9/5oQ/fSncrBPG5aGGFAAAAABQYrEgBQAAAAC4ggUpAAAAAMAVLEgBAAAAAK4omjsZAwAAAMBlunDhgtu7gGLGFVIAAAAAgCtYkAIAAAAAXMGCFAAAAADgCo/DHXwBAAAAAC7gCikAAAAAwBUsSAEAAAAArmBBCgAAAABwBQtSAAAAAIArWJACAAAAAFzBghQAAAAA4AoWpAAAAAAAV7AgBQAAAAC4ggUpAAAAAMAV/wfbGHZQGBdaigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes:\n",
      "MNIST: 10\n",
      "EMNIST: 26\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes\n",
    "print(\"MNIST Dataset:\")\n",
    "print(f\"Training set size: {len(mnist_trainset)}\")\n",
    "print(f\"Validation set size: {len(mnist_valset)}\")\n",
    "print(f\"Test set size: {len(mnist_testset)}\")\n",
    "print(\"\\nEMNIST Dataset:\")\n",
    "print(f\"Training set size: {len(emnist_trainset)}\")\n",
    "print(f\"Validation set size: {len(emnist_valset)}\")\n",
    "print(f\"Test set size: {len(emnist_testset)}\")\n",
    "\n",
    "# Visualize some examples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to show images\n",
    "def show_examples(dataset, title, num_examples=5, mnist=True):\n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(2*num_examples, 2))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        img, label = dataset[i]\n",
    "        if not mnist:\n",
    "            label = chr(ord('A') + label - 1)  # Convert number to corresponding letter\n",
    "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show examples from both datasets\n",
    "show_examples(mnist_full_train, \"MNIST Examples\")\n",
    "show_examples(emnist_full_train, \"EMNIST Examples\", mnist=False)\n",
    "\n",
    "# Print number of classes\n",
    "print(\"\\nNumber of classes:\")\n",
    "print(f\"MNIST: {len(set(mnist_full_train.targets.numpy()))}\")\n",
    "print(f\"EMNIST: {len(set(emnist_full_train.targets.numpy()))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (l1): Linear(in_features=784, out_features=2000, bias=True)\n",
       "  (l2): Linear(in_features=2000, out_features=1500, bias=True)\n",
       "  (l3): Linear(in_features=1500, out_features=1000, bias=True)\n",
       "  (l4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (l5): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (l6): Linear(in_features=250, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as L\n",
    "\n",
    "class SimpleNN(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.l1 = nn.Linear(28*28, 2000)\n",
    "        self.l2 = nn.Linear(2000, 1500)\n",
    "        self.l3 = nn.Linear(1500, 1000)\n",
    "        self.l4 = nn.Linear(1000, 500)\n",
    "        self.l5 = nn.Linear(500, 250)\n",
    "        self.l6 = nn.Linear(250, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.l5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "base_model = SimpleNN()\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/i551965/Documents/dev/SAP/DeepLearning/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | l1      | Linear  | 1.6 M  | train\n",
      "1 | l2      | Linear  | 3.0 M  | train\n",
      "2 | l3      | Linear  | 1.5 M  | train\n",
      "3 | l4      | Linear  | 500 K  | train\n",
      "4 | l5      | Linear  | 125 K  | train\n",
      "5 | l6      | Linear  | 2.5 K  | train\n",
      "6 | dropout | Dropout | 0      | train\n",
      "--------------------------------------------\n",
      "6.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 M     Total params\n",
      "26.803    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/SAP/DeepLearning/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/SAP/DeepLearning/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [00:08<00:00, 90.10it/s, v_num=66] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [00:08<00:00, 89.37it/s, v_num=66]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=3)\n",
    "trainer.fit(base_model, mnist_train_loader, mnist_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/SAP/DeepLearning/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 156.98it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc             0.97079998254776\n",
      "        val_loss            0.10735946148633957\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "EMNIST\n",
      "Validation DataLoader 0: 100%|██████████| 325/325 [00:01<00:00, 202.30it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc           0.014759615063667297\n",
      "        val_loss            3.2453672885894775\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.2453672885894775, 'val_acc': 0.014759615063667297}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"MNIST\")\n",
    "trainer.validate(model=base_model, dataloaders=mnist_test_loader)\n",
    "print(\"EMNIST\")\n",
    "trainer.validate(model=base_model, dataloaders=emnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(L.LightningModule):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device=device):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper: \n",
    "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        \n",
    "        # Section 4.1 of the paper: \n",
    "        #   We then scale ∆Wx by α/r , where α is a constant in r. \n",
    "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n",
    "        #   As a result, we simply set α to the first r we try and do not tune it. \n",
    "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        # Return W + (B*A)*scale\n",
    "        return original_weights + (self.lora_B @ self.lora_A) * self.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add parametrization to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (l1): ParametrizedLinear(\n",
       "    in_features=784, out_features=2000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (l2): ParametrizedLinear(\n",
       "    in_features=2000, out_features=1500, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (l3): ParametrizedLinear(\n",
       "    in_features=1500, out_features=1000, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (l4): ParametrizedLinear(\n",
       "    in_features=1000, out_features=500, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (l5): ParametrizedLinear(\n",
       "    in_features=500, out_features=250, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (l6): ParametrizedLinear(\n",
       "    in_features=250, out_features=10, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "import copy\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "\n",
    "    # From section 4.2 of the paper:\n",
    "    #   We limit our study to only adapting the attention weights for downstream tasks \n",
    "    #   and freeze the MLP modules (so they are not trained in downstream tasks) both \n",
    "    #   for simplicity and parameter-efficiency.\n",
    "    #   [...]\n",
    "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
    "    \n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "# Creating a copy of the base model to finetune without LoRA\n",
    "lora_finetune_model = copy.deepcopy(base_model)\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    lora_finetune_model.l1, \"weight\", linear_layer_parameterization(lora_finetune_model.l1, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    lora_finetune_model.l2, \"weight\", linear_layer_parameterization(lora_finetune_model.l2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    lora_finetune_model.l3, \"weight\", linear_layer_parameterization(lora_finetune_model.l3, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    lora_finetune_model.l4, \"weight\", linear_layer_parameterization(lora_finetune_model.l4, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    lora_finetune_model.l5, \"weight\", linear_layer_parameterization(lora_finetune_model.l5, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    lora_finetune_model.l6, \"weight\", linear_layer_parameterization(lora_finetune_model.l6, device)\n",
    ")\n",
    "\n",
    "lora_finetune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze all non-LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter l1.bias\n",
      "Freezing non-LoRA parameter l1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter l2.bias\n",
      "Freezing non-LoRA parameter l2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter l3.bias\n",
      "Freezing non-LoRA parameter l3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter l4.bias\n",
      "Freezing non-LoRA parameter l4.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter l5.bias\n",
      "Freezing non-LoRA parameter l5.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter l6.bias\n",
      "Freezing non-LoRA parameter l6.parametrizations.weight.original\n"
     ]
    }
   ],
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in lora_finetune_model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that parameters are actually frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 11294\n",
      "Total parameters: 6712054\n",
      "Percentage of trainable parameters: 0.17%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in lora_finetune_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in lora_finetune_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model on EMNIST using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | l1      | ParametrizedLinear | 1.6 M  | train\n",
      "1 | l2      | ParametrizedLinear | 3.0 M  | train\n",
      "2 | l3      | ParametrizedLinear | 1.5 M  | train\n",
      "3 | l4      | ParametrizedLinear | 502 K  | train\n",
      "4 | l5      | ParametrizedLinear | 126 K  | train\n",
      "5 | l6      | ParametrizedLinear | 2.8 K  | train\n",
      "6 | dropout | Dropout            | 0      | train\n",
      "-------------------------------------------------------\n",
      "11.3 K    Trainable params\n",
      "6.7 M     Non-trainable params\n",
      "6.7 M     Total params\n",
      "26.848    Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1560/1560 [00:17<00:00, 87.03it/s, v_num=67]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1560/1560 [00:17<00:00, 86.86it/s, v_num=67]\n",
      "Training time with LoRA: 55.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import pendulum\n",
    "\n",
    "trainer = L.Trainer(max_epochs=3)\n",
    "# Measure training time\n",
    "start_time = pendulum.now()\n",
    "trainer.fit(lora_finetune_model, emnist_train_loader, emnist_val_loader)\n",
    "training_time_lora = pendulum.now().diff(start_time).in_seconds()\n",
    "print(f\"Training time with LoRA: {training_time_lora:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n",
      "Validation DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 173.84it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.13099999725818634\n",
      "        val_loss            3.4808096885681152\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "EMNIST\n",
      "Validation DataLoader 0: 100%|██████████| 325/325 [00:01<00:00, 187.75it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.25370192527770996\n",
      "        val_loss            0.29321321845054626\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST\")\n",
    "accuracy_lora_mnist = trainer.validate(model=lora_finetune_model, dataloaders=mnist_test_loader)\n",
    "print(\"EMNIST\")\n",
    "accuracy_lora_emnist = trainer.validate(model=lora_finetune_model, dataloaders=emnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune without LoRA to compare accuracy and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | l1      | Linear  | 1.6 M  | train\n",
      "1 | l2      | Linear  | 3.0 M  | train\n",
      "2 | l3      | Linear  | 1.5 M  | train\n",
      "3 | l4      | Linear  | 500 K  | train\n",
      "4 | l5      | Linear  | 125 K  | train\n",
      "5 | l6      | Linear  | 2.5 K  | train\n",
      "6 | dropout | Dropout | 0      | train\n",
      "--------------------------------------------\n",
      "6.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 M     Total params\n",
      "26.803    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1560/1560 [00:18<00:00, 84.54it/s, v_num=68]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1560/1560 [00:18<00:00, 84.22it/s, v_num=68]\n",
      "Training time without LoRA: 53.00 seconds\n",
      "MNIST\n",
      "Validation DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 207.33it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.0828000009059906\n",
      "        val_loss             6.055171966552734\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "EMNIST\n",
      "Validation DataLoader 0: 100%|██████████| 325/325 [00:01<00:00, 200.87it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.32442307472229004\n",
      "        val_loss            0.07956093549728394\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# Finetune the base model without LoRA\n",
    "no_lora_finetune_model = copy.deepcopy(base_model)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=3)\n",
    "\n",
    "start_time = pendulum.now()\n",
    "trainer.fit(no_lora_finetune_model, emnist_train_loader, emnist_val_loader)\n",
    "training_time_no_lora = pendulum.now().diff(start_time).in_seconds()\n",
    "print(f\"Training time without LoRA: {training_time_no_lora:.2f} seconds\")\n",
    "\n",
    "### Evaluate the finetuned model without LoRA\n",
    "print(\"MNIST\")\n",
    "accuracy_no_lora_mnist = trainer.validate(model=no_lora_finetune_model, dataloaders=mnist_test_loader)\n",
    "print(\"EMNIST\")\n",
    "accuracy_no_lora_emnist = trainer.validate(model=no_lora_finetune_model, dataloaders=emnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Finetuning with and without LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time with LoRA: 55.00 seconds\n",
      "Training time without LoRA: 53.00 seconds\n",
      "\n",
      "Accuracy with LoRA on MNIST: 0.13\n",
      "Accuracy with LoRA on EMNIST: 0.25\n",
      "Accuracy without LoRA on MNIST: 0.08\n",
      "Accuracy without LoRA on EMNIST: 0.32\n"
     ]
    }
   ],
   "source": [
    "# Training time\n",
    "print(f\"Training time with LoRA: {training_time_lora:.2f} seconds\")\n",
    "print(f\"Training time without LoRA: {training_time_no_lora:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Accuracy on both datasets\n",
    "print(f\"Accuracy with LoRA on MNIST: {accuracy_lora_mnist[0]['val_acc']:.2f}\")\n",
    "print(f\"Accuracy with LoRA on EMNIST: {accuracy_lora_emnist[0]['val_acc']:.2f}\")\n",
    "print(f\"Accuracy without LoRA on MNIST: {accuracy_no_lora_mnist[0]['val_acc']:.2f}\")\n",
    "print(f\"Accuracy without LoRA on EMNIST: {accuracy_no_lora_emnist[0]['val_acc']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
